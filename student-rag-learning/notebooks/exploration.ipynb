{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qlegr\\.pyenv\\pyenv-win\\versions\\3.12.3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentChunker:\n",
    "    def __init__(self, chunk_size: int = 500):\n",
    "        self.chunk_size = chunk_size\n",
    "    \n",
    "    @staticmethod\n",
    "    def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between two vectors.\n",
    "        \n",
    "        Args:\n",
    "            v1 (np.ndarray): First vector\n",
    "            v2 (np.ndarray): Second vector\n",
    "            \n",
    "        Returns:\n",
    "            float: Cosine similarity score between -1 and 1\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        norm1 = np.linalg.norm(v1)\n",
    "        norm2 = np.linalg.norm(v2)\n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "    def chunk_by_semantic_similarity(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk text based on semantic similarity using sentence embeddings.\n",
    "        More efficient version that computes embeddings only once per sentence.\n",
    "        \"\"\"\n",
    "        # Split into sentences and filter empty ones\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        \n",
    "        # Initialize sentence transformer\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Get embeddings for all sentences at once (more efficient)\n",
    "        embeddings = model.encode(sentences)\n",
    "        \n",
    "        chunks = []\n",
    "        chunks_embeddings = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        # Zip embeddings with their corresponding sentences and next embeddings\n",
    "        for sentence, next_emb in zip(sentences,embeddings[1:], strict=False):\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += len(sentence)\n",
    "            \n",
    "            if current_length >= self.chunk_size:\n",
    "                curr_emb = model.encode('. '.join(current_chunk) + '.')\n",
    "                similarity = self.cosine_similarity(curr_emb, next_emb)\n",
    "                \n",
    "                if similarity < 0.5:\n",
    "                    chunks.append('. '.join(current_chunk) + '.')\n",
    "                    chunks_embeddings.append(curr_emb)\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "        \n",
    "        # Add remaining sentences as the last chunk\n",
    "        if current_chunk:\n",
    "            last_chunk = '. '.join(current_chunk) + '.'\n",
    "            chunks.append(last_chunk)\n",
    "            chunks_embeddings.append(model.encode(last_chunk))\n",
    "\n",
    "        return chunks, chunks_embeddings\n",
    "\n",
    "    def create_document_chunks(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create chunks with metadata\n",
    "        \"\"\"\n",
    "        chunks, embeddings = self.chunk_by_semantic_similarity(text)\n",
    "            \n",
    "        # Add metadata to chunks\n",
    "        doc_chunks = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            doc_chunks.append({\n",
    "                'content': chunk,\n",
    "                'embedding': embedding,\n",
    "                'metadata': {\n",
    "                    'chunk_id': i,\n",
    "                    'chunk_size': len(chunk),\n",
    "                    'chunk_method': 'semantic'\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        return doc_chunks\n",
    "\n",
    "# Usage example\n",
    "def process_pdf_to_chunks(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process a PDF file and return chunked content\n",
    "    \"\"\"\n",
    "    # Read PDF\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    # Create chunks\n",
    "    chunker = DocumentChunker(chunk_size=500)\n",
    "    chunks = chunker.create_document_chunks(text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = os.environ[\"DATA_ROOT\"]\n",
    "DATA_FOLDER = \"student-rag\"\n",
    "FILE = \"Designing Machine Learning Systems.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = os.path.join(DATA_ROOT, DATA_FOLDER, FILE)\n",
    "semantic_chunks = process_pdf_to_chunks(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_lenghts = [chunk['metadata'][\"chunk_size\"] for chunk in semantic_chunks]\n",
    "pd.DataFrame(chunks_lenghts).describe([0.9, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "def store_chunks_in_vectordb(chunks: List[Dict], collection_name: str = \"pdf_chunks\"):\n",
    "    \"\"\"\n",
    "    Store chunks in ChromaDB with explicit embedding function\n",
    "    \"\"\"\n",
    "    # Initialize ChromaDB\n",
    "    client = Client()\n",
    "    \n",
    "    # Initialize the default embedding function (what ChromaDB uses under the hood)\n",
    "    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "        model_name=\"all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    # Create or get collection with explicit embedding function\n",
    "    collection = client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=embedding_fn\n",
    "    )\n",
    "    \n",
    "    # Prepare documents for insertion\n",
    "    documents = [chunk['content'] for chunk in chunks]\n",
    "    ids = [str(chunk['metadata']['chunk_id']) for chunk in chunks]\n",
    "    metadatas = [chunk['metadata'] for chunk in chunks]\n",
    "    \n",
    "    # Add documents to collection\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = store_chunks_in_vectordb(semantic_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key components of a machine learning system?\n",
      "\n",
      "Relevant chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Metadata: {'chunk_id': 131, 'chunk_method': 'semantic', 'chunk_size': 555}\n",
      "Content: We’ll provide detail on what this holistic\n",
      "approach means in the next chapter. Summary | 23CHAPTER 2\n",
      "Introduction to Machine Learning\n",
      "Systems Design\n",
      "Now that we’ve walked through an overview of ML sys...\n",
      "\n",
      "Chunk 2:\n",
      "Metadata: {'chunk_id': 1, 'chunk_method': 'semantic', 'chunk_size': 556}\n",
      "Content: 99  CAN $74. 99\n",
      "ISBN: 978-1-098-10796-3Twitter: @oreillymedia\n",
      "linkedin. com/company/oreilly-media\n",
      "youtube. com/oreillymedia Machine learning systems are both complex and unique. Complex because they c...\n",
      "\n",
      "Chunk 3:\n",
      "Metadata: {'chunk_id': 50, 'chunk_method': 'semantic', 'chunk_size': 546}\n",
      "Content: Figure 1-1. Different  components of an ML system. “ML algorithms” is usually what\n",
      "people think of when they say machine learning, but it’s only a small part of the entire\n",
      "system. There are many excel...\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"What are the key components of a machine learning system?\"\n",
    "\n",
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3  # Number of chunks to return\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nRelevant chunks:\")\n",
    "for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Metadata: {metadata}\")\n",
    "    print(f\"Content: {doc[:200]}...\")  # Print first 200 chars of each chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
